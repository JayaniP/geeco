{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyframe Extraction\n",
    "Extracts keyframes and goal images from all tfrecords in a GEECO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''imports'''\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.geeco_gym import PickAndPlaceEncodingV4, PickAndPlaceMetaV4\n",
    "from utils.plotting import create_image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''path setup'''\n",
    "root_path = os.environ['GEECO_ROOT']\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-push-pad1-cube1-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-push-pad1-cube2-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-push-pad2-cube1-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-push-pad2-cube2-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-pick-pad1-cube1-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-pick-pad1-cube2-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-pick-pad2-cube1-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-pick-pad2-cube2-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-pick-pad2-cube2-clutter4-v4')\n",
    "# dataset_dir = os.path.join(root_path, 'data', 'gym-pick-pad2-cube2-clutter12-v4')\n",
    "print(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''collect files'''\n",
    "# tfrecord paths\n",
    "tfrecord_dir = os.path.join(dataset_dir, 'data')\n",
    "tfrecord_files = [f for f in os.listdir(tfrecord_dir) if f.endswith('.tfrecord.zlib')]\n",
    "tfrecord_files.sort()\n",
    "tfrecord_paths = [os.path.join(tfrecord_dir, f) for f in tfrecord_files]\n",
    "# meta information\n",
    "meta_info_path = os.path.join(dataset_dir, 'meta', 'meta_info.json')\n",
    "with open(meta_info_path, 'r') as fp:\n",
    "    meta_info_dict = json.load(fp)\n",
    "meta = PickAndPlaceMetaV4(**meta_info_dict)\n",
    "# quick check output\n",
    "print(len(tfrecord_files))\n",
    "pprint.pprint(tfrecord_files[:10])\n",
    "pprint.pprint(meta_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''parsing function'''\n",
    "def _parse_record_v4(proto_example, meta):\n",
    "    encoding = PickAndPlaceEncodingV4(meta)\n",
    "    context_decoder, sequence_decoder = encoding.decode()\n",
    "    # parse proto example\n",
    "    context_data, sequence_data = tf.parse_single_sequence_example(\n",
    "        serialized=proto_example,\n",
    "        context_features=context_decoder,\n",
    "        sequence_features=sequence_decoder)\n",
    "    parsed_example = {}\n",
    "    # parsed_example.update(context_data)  # context == meta here!\n",
    "    parsed_example.update(sequence_data)\n",
    "    # reshape data fields\n",
    "    rgb = parsed_example['rgb']\n",
    "    parsed_example['rgb'] = tf.reshape(rgb, [-1, meta.img_height, meta.img_width, 3])\n",
    "    depth = parsed_example['depth']\n",
    "    parsed_example['depth'] = tf.reshape(depth, [-1, meta.img_height, meta.img_width, 1])\n",
    "    # normalize data\n",
    "    parsed_example['rgb'] /= 255.0  # RGB recorded as uint8 [0 .. 255]\n",
    "    return parsed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''build dataset graph'''\n",
    "num_threads = 4\n",
    "# tfrecord dataset from sorted paths\n",
    "dataset = tf.data.TFRecordDataset(\n",
    "    filenames=tfrecord_paths,\n",
    "    compression_type='ZLIB',\n",
    "    num_parallel_reads=num_threads)\n",
    "dataset = dataset.map(\n",
    "    lambda proto_example: _parse_record_v4(proto_example, meta),\n",
    "    num_parallel_calls=num_threads)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "data = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''prepare output directories'''\n",
    "rgb_targets_dir = os.path.join(dataset_dir, 'images', 'targets', 'rgb')\n",
    "depth_targets_dir = os.path.join(dataset_dir, 'images', 'targets', 'depth')\n",
    "rgb_keyframes_dir = os.path.join(dataset_dir, 'images', 'keyframes', 'rgb')\n",
    "depth_keyframes_dir = os.path.join(dataset_dir, 'images', 'keyframes', 'depth')\n",
    "for out_dir in [rgb_targets_dir, depth_targets_dir, rgb_keyframes_dir, depth_keyframes_dir]:\n",
    "    os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''session setup'''\n",
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loop over tfrecord_paths and extract targets from corresponding records'''\n",
    "for p in tqdm(tfrecord_paths):\n",
    "    d = sess.run(data)\n",
    "    filename = os.path.basename(p).split('.')[0]\n",
    "    # extract RGB target and write to file\n",
    "    rgb_target = np.squeeze(d['rgb'][-1])\n",
    "    rgb_target_path = os.path.join(rgb_targets_dir, filename + '.png')\n",
    "    # scipy.misc.imsave(rgb_target_path, rgb_target)\n",
    "    Image.fromarray((rgb_target * 255).astype(np.uint8)).save(rgb_target_path)\n",
    "    # rgb_import = scipy.misc.imread(rgb_target_path) / 255.0\n",
    "    rgb_import = np.array(Image.open(rgb_target_path), dtype=np.float32) / 255.0\n",
    "    try:\n",
    "        assert np.allclose(rgb_target, rgb_import)\n",
    "    except AssertionError as err:\n",
    "        print(\">>> Faulty RGB export for %s\" % filename)\n",
    "    # extract depth target and write to file\n",
    "    depth_target = np.squeeze(d['depth'][-1])\n",
    "    depth_target_path = os.path.join(depth_targets_dir, filename + '.npy')\n",
    "    np.save(depth_target_path, depth_target)\n",
    "    depth_import = np.load(depth_target_path)\n",
    "    try:\n",
    "        assert np.allclose(depth_target, depth_import)\n",
    "    except AssertionError as err:\n",
    "        print(\">>> Faulty depth export for %s\" % filename)\n",
    "    # look up corresponding keyframe file; skip this part, if it does not exist\n",
    "    record_id = re.search(r'\\d+', filename).group(0)\n",
    "    keyframe_filename = 'key_frames_%s.json' % (record_id, )\n",
    "    keyframe_file = os.path.join(dataset_dir, 'data', keyframe_filename)\n",
    "    if os.path.exists(keyframe_file):  # extract keyframes specified\n",
    "        with open(keyframe_file) as fp:\n",
    "            keyframe_dict = json.load(fp)\n",
    "        keyframe_indices = keyframe_dict['key_frames']\n",
    "        for key_idx in keyframe_indices:\n",
    "#             key_idx = np.min([key_idx+1, len(d['rgb'])-1])  # adjusting idx by +1 to ensure that things have settled\n",
    "            key_idx = np.min([key_idx, len(d['rgb'])-1])  # adjusting idx by +1 to ensure that things have settled\n",
    "            # extract RGB target and write to file\n",
    "            rgb_target = np.squeeze(d['rgb'][key_idx])\n",
    "            rgb_target_path = os.path.join(rgb_keyframes_dir, filename + '_%04d' % key_idx + '.png')\n",
    "            # scipy.misc.imsave(rgb_target_path, rgb_target)\n",
    "            Image.fromarray((rgb_target * 255).astype(np.uint8)).save(rgb_target_path)\n",
    "            # rgb_import = scipy.misc.imread(rgb_target_path) / 255.0\n",
    "            rgb_import = np.array(Image.open(rgb_target_path), dtype=np.float32) / 255.0\n",
    "            try:\n",
    "                assert np.allclose(rgb_target, rgb_import)\n",
    "            except AssertionError as err:\n",
    "                print(\">>> Faulty RGB export for %s\" % filename)\n",
    "            # extract depth target and write to file\n",
    "            depth_target = np.squeeze(d['depth'][key_idx])\n",
    "            depth_target_path = os.path.join(depth_keyframes_dir, filename + '_%04d' % key_idx + '.npy')\n",
    "            np.save(depth_target_path, depth_target)\n",
    "            depth_import = np.load(depth_target_path)\n",
    "            try:\n",
    "                assert np.allclose(depth_target, depth_import)\n",
    "            except AssertionError as err:\n",
    "                print(\">>> Faulty depth export for %s\" % filename)\n",
    "    else:  # no keyframes specified, move on\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
